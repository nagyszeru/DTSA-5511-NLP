{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Brief description of the problem and data**","metadata":{}},{"cell_type":"markdown","source":"This Kaggle competition is about classifying texts taken from Twitter using Natural Language Processing (NLP) to classify which are about real disasters and which are not. \n\nThe complexity of the task is demonstrated using the sample Tweet provided by the competition: \"LOOK AT THE NIGHT SKY LAST NIGHT IT WAS ABLAZE\" where the author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away but it is less clear to a machine that it is not referencing a real disaster.\n\nThe dataset consists of 10,000 tweets that were hand classified.","metadata":{}},{"cell_type":"markdown","source":"# **Load Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport os\n#import random\n#import re\n#from collections import defaultdict\nimport wordcloud\n#from wordcloud import WordCloud\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom functools import reduce\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data**","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\ntest_data = pd.read_csv('../input/nlp-getting-started/test.csv')\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['target'].value_counts().to_frame()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby(['target'])['target'].count().plot(kind='bar', color = 'blue', title='Target Distribution')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of words in a tweet\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len = train_data[train_data['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len, color='black')\nax1.set_title('Disaster tweets')\ntweet_len = train_data[train_data['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='purple')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a tweet')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_cases = \"\".join(train_data[train_data['target'] == 0]['text'].values)\nnegative_cases = \"\".join(train_data[train_data['target'] == 1]['text'].values)\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 8))\n\nwc1 = WordCloud(background_color='white').generate(positive_cases)\nwc2 = WordCloud(background_color='white').generate(negative_cases)\n\naxs[0].imshow(wc1, interpolation='bilinear')\naxs[0].set_title('Positive')\naxs[1].imshow(wc2, interpolation='bilinear')\naxs[1].set_title('Negative')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed = 777\nos.environ['PYTHONHASHSEED']=str(seed)\ntf.random.set_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#remove punctuation\ndef remove_punctuation(x):\n    return x.translate(str.maketrans('', '', string.punctuation))\n\n#remove stopwords\ndef remove_stopwords(x):\n    return ' '.join([i for i in x.split() if i not in wordcloud.STOPWORDS])\n\n#remove words less than 4 \ndef remove_less_than(x):\n    return ' '.join([i for i in x.split() if len(i) > 3])\n\n#remove words with non-alphabet characters\ndef remove_non_alphabet(x):\n    return ' '.join([i for i in x.split() if i.isalpha()])\n\n#def strip_all_entities(x):\n#    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\",\" \",x).split())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['clean_text'] = train_data['text'].apply(lambda x: x.lower())\n#train_data['text2'] = train_data['text2'].apply(lambda x: re.sub(r'https?://\\S+|www\\.\\S+', '', x, flags = re.MULTILINE))\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_less_than)\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_non_alphabet)\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_stopwords)\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_punctuation)\n#train_data['text2'] = train_data['text2'].apply(spell_check)\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_cases = \"\".join(train_data[train_data['target'] == 0]['clean_text'].values)\nnegative_cases = \"\".join(train_data[train_data['target'] == 1]['clean_text'].values)\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 8))\n\nwc1 = WordCloud(background_color='white').generate(positive_cases)\nwc2 = WordCloud(background_color='white').generate(negative_cases)\n\naxs[0].imshow(wc1, interpolation='bilinear')\naxs[0].set_title('Positive')\naxs[1].imshow(wc2, interpolation='bilinear')\naxs[1].set_title('Negative')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Build Model**","metadata":{}},{"cell_type":"code","source":"disaster_keywords = train_data.loc[train_data[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train_data.loc[train_data[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:20].index, x=disaster_keywords[0:20], orient='h', ax=ax[0], palette=\"Reds_d\")\nax[0].set_title(\"Top 20 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\n\nsns.barplot(y=nondisaster_keywords[0:20].index, x=nondisaster_keywords[0:20], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[1].set_title(\"Top 20 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_disaster_keyword = train_data.groupby('keyword').mean()['target'].sort_values(ascending = False).head(20)\ntop_nondisaster_keyword = train_data.groupby('keyword').mean()['target'].sort_values().head(20)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\n\nsns.barplot(y=top_disaster_keyword[0:20].index, x=disaster_keywords[0:20], orient='h', ax=ax[0], palette=\"Reds_d\")\nax[0].set_title(\"Top 20 Keywords - Highest used Disaster Keyword\")\nax[0].set_xlabel(\"Keyword Frequency\")\n\n\nsns.barplot(y=top_nondisaster_keyword[0:20].index, x=top_nondisaster_keyword[0:20], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[1].set_title(\"Top 20 Keywords - Least used Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\n\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"locations = train_data[\"location\"].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,7))\nsns.barplot(y=locations[0:20].index, x=locations[0:20], orient='h')\nplt.title(\"Top 20 Locations\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(train_data['text']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[\"length\"]  = train_data[\"text\"].apply(len)\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['length'].describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(5,3))\nsns.histplot(train_data[\"length\"], kde=True,color='purple',bins=30)\nplt.title(\"Length of tweets\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.hist(column='length', by = 'target',bins =60, figsize= (10,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def count_words(x):\n    return len(x.split())\n\ntrain_data[\"num_words\"] = train_data[\"text\"].apply(count_words)\n\nplt.figure(figsize=(5,3))\nsns.histplot(train_data[\"num_words\"],kde=True,color='purple',bins=30)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.hist(column='num_words', by = 'target',bins =60, figsize= (10,3))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features=3000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(train_data['clean_text'].values)\nX = tokenizer.texts_to_sequences(train_data['clean_text'].values)\nX = pad_sequences(X)\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state =41)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n# reference: https://www.tensorflow.org/xla\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 32\nlstm_out = 32\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.4))\nmodel.add(Dense(1,activation='sigmoid'))\nadam = optimizers.Adam(learning_rate=0.002)\nmodel.compile(loss = 'binary_crossentropy', optimizer=adam ,metrics = ['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, epochs = 10, batch_size=32, validation_data=(X_test, y_test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model.predict(X_test).round()\ny_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_accuracy = round(metrics.accuracy_score(y_train,model.predict(X_train).round())*100)\ntrain_accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Accuracy  is  : ', (metrics.accuracy_score(y_test, y_pred)))\nprint('Recall  is    : ', (metrics.recall_score(y_test, y_pred)))\nprint('Precision  is : ', (metrics.precision_score(y_test, y_pred)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"conm = confusion_matrix(y_test,y_pred)\nplt.figure(figsize=(7, 5))\nsns.heatmap(conm, annot=True, fmt='d', cmap='cool')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/nlp-getting-started/test.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.head().style.background_gradient(cmap='coolwarm')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['clean_text'] = test_data['text'].apply(lambda x: x.lower())\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_less_than)\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_non_alphabet)\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_stopwords)\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_punctuation)\nprint('done')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l =50\nmax_features=5000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(train_data['clean_text'].values)\nX = tokenizer.texts_to_sequences(train_data['clean_text'].values)\nX = pad_sequences(X, maxlen =l)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(train_data['clean_text'].values)\ntest_token = tokenizer.texts_to_sequences(test_data['clean_text'].values)\ntest_token = pad_sequences(test_token, maxlen =l)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 100\nlstm_out = 100\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(lstm_out, dropout=0.2, return_sequences=True,recurrent_dropout=0.4))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(lstm_out,dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nadam = optimizers.Adam(learning_rate=2e-3)\nmodel.compile(loss = 'binary_crossentropy', optimizer=adam ,metrics = ['accuracy'])\nprint(model.summary())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\nmodel.fit(X,y, epochs = 10,validation_split = 0.2 ,callbacks=[es_callback], batch_size=32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat = model.predict(test_token).round()\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = np.round(y_hat).astype('int')\nsubmission.to_csv('submission.csv', index=False)\nsubmission.describe().style.background_gradient(cmap='coolwarm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.target.value_counts().plot.bar();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}