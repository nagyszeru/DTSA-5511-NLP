{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Brief description of the problem and data**","metadata":{}},{"cell_type":"markdown","source":"This Kaggle competition is about classifying texts taken from Twitter using Natural Language Processing (NLP) to classify which are about real disasters and which are not. The complexity of the task is demonstrated using the sample Tweet provided by the competition: \"LOOK AT THE NIGHT SKY LAST NIGHT IT WAS ABLAZE\" where the author explicitly uses the word “ABLAZE” but means it metaphorically. This is clear to a human right away but it is less clear to a machine that it is not referencing a real disaster.\n\nThe dataset consists of 10,000 tweets that were hand classified.","metadata":{}},{"cell_type":"markdown","source":"# **Load Libraries**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport os\nimport random\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout\nfrom tensorflow.keras.utils import plot_model\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n#from functools import reduce\n#from nltk.stem import PorterStemmer, WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:38:22.249414Z","iopub.execute_input":"2023-04-03T20:38:22.250683Z","iopub.status.idle":"2023-04-03T20:38:31.798313Z","shell.execute_reply.started":"2023-04-03T20:38:22.250628Z","shell.execute_reply":"2023-04-03T20:38:31.797171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data**","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('../input/nlp-getting-started/train.csv')\ntrain_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:39.887671Z","iopub.execute_input":"2023-04-03T20:40:39.888798Z","iopub.status.idle":"2023-04-03T20:40:39.963994Z","shell.execute_reply.started":"2023-04-03T20:40:39.888738Z","shell.execute_reply":"2023-04-03T20:40:39.962761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data = pd.read_csv('../input/nlp-getting-started/test.csv')\ntest_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:42.233464Z","iopub.execute_input":"2023-04-03T20:40:42.233885Z","iopub.status.idle":"2023-04-03T20:40:42.266823Z","shell.execute_reply.started":"2023-04-03T20:40:42.233845Z","shell.execute_reply":"2023-04-03T20:40:42.265693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:45.067547Z","iopub.execute_input":"2023-04-03T20:40:45.067968Z","iopub.status.idle":"2023-04-03T20:40:45.096984Z","shell.execute_reply.started":"2023-04-03T20:40:45.067927Z","shell.execute_reply":"2023-04-03T20:40:45.095700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data.info()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:47.945697Z","iopub.execute_input":"2023-04-03T20:40:47.946103Z","iopub.status.idle":"2023-04-03T20:40:47.960650Z","shell.execute_reply.started":"2023-04-03T20:40:47.946066Z","shell.execute_reply":"2023-04-03T20:40:47.959219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From a high level look at the datasets it is observed:\n\n* The training set contains 7,613 entries while the testing dataset contains 3,263 entries\n* Both datasets contain 4 columns: ID, Keyword, Location, Text\n* The training dataset contains 1 additional column which we will be predicting for the test dataset: Target\n\nWe will evaluate the fields further below.\n","metadata":{}},{"cell_type":"code","source":"train_data['target'].value_counts().to_frame()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:50.779118Z","iopub.execute_input":"2023-04-03T20:40:50.779561Z","iopub.status.idle":"2023-04-03T20:40:50.792889Z","shell.execute_reply.started":"2023-04-03T20:40:50.779523Z","shell.execute_reply":"2023-04-03T20:40:50.791429Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.groupby(['target'])['target'].count().plot(kind='bar', color = 'blue', title='Target Distribution')\nplt.xlabel('Target')\nplt.ylabel('Count')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:53.393266Z","iopub.execute_input":"2023-04-03T20:40:53.393659Z","iopub.status.idle":"2023-04-03T20:40:53.671060Z","shell.execute_reply.started":"2023-04-03T20:40:53.393623Z","shell.execute_reply":"2023-04-03T20:40:53.669767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['keyword'].value_counts()[:10].plot(kind='barh', color='blue')\nplt.title(\"Keywords - Top 10\")\nplt.xlabel(\"Count\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:56.532678Z","iopub.execute_input":"2023-04-03T20:40:56.533964Z","iopub.status.idle":"2023-04-03T20:40:56.793738Z","shell.execute_reply.started":"2023-04-03T20:40:56.533917Z","shell.execute_reply":"2023-04-03T20:40:56.792507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['location'].value_counts()[:10].plot(kind='barh', color='blue')\nplt.title(\"Location - Top 10\")\nplt.xlabel(\"Count\")","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:40:59.752438Z","iopub.execute_input":"2023-04-03T20:40:59.753563Z","iopub.status.idle":"2023-04-03T20:40:59.969382Z","shell.execute_reply.started":"2023-04-03T20:40:59.753520Z","shell.execute_reply":"2023-04-03T20:40:59.968265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* From the above analysis of the training dataset, we can see approximately 60% of the Tweets reference a non-disaster (target=0) versus a disaster (target=1).\n* The keywords field contains a word that represents the overall text.\n* The location field contains a geographical location from which the text originated, in some cases a country name while in others a city location.  \n* We can consider cleaning up the keyword and location for empty value or standardization yet they will not directly be used in the NLP model below so will defer doing so within the scope of this project.\n\nNext, we will examine the primary field for our analysis which will be the 'text' derived from the Tweets.","metadata":{}},{"cell_type":"code","source":"text_length = train_data[train_data['target']==1]['text'].str.split().map(lambda x: len(x))\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\nfig.suptitle('WORDS IN A TWEET')\nax1.hist(text_length, color='red', edgecolor='black')\nax1.set_title('Tweets Disaster')\nax1.set_xlabel('# Words')\nax1.set_ylabel('Frequency')\ntext_length = train_data[train_data['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_length,color='blue', edgecolor='black')\nax2.set_title('Tweets Non-Disaster')\nax2.set_xlabel('# Words')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:03.459280Z","iopub.execute_input":"2023-04-03T20:41:03.459892Z","iopub.status.idle":"2023-04-03T20:41:03.868378Z","shell.execute_reply.started":"2023-04-03T20:41:03.459854Z","shell.execute_reply":"2023-04-03T20:41:03.866996Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Above we have a histogram of the average number of words in each text for both disaster and non-disaster related Tweets.\nIn both cases, the average number of words tends to be approximately 15 with more variance in the average for disaster versus non-disaster tweets.","metadata":{}},{"cell_type":"code","source":"positive_cases = \"\".join(train_data[train_data['target'] == 0]['text'].values)\nnegative_cases = \"\".join(train_data[train_data['target'] == 1]['text'].values)\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 8))\n\nwc1 = WordCloud(background_color='white').generate(positive_cases)\nwc2 = WordCloud(background_color='white').generate(negative_cases)\n\naxs[0].imshow(wc1, interpolation='bilinear')\naxs[0].set_title('Positive')\naxs[1].imshow(wc2, interpolation='bilinear')\naxs[1].set_title('Negative')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:08.472024Z","iopub.execute_input":"2023-04-03T20:41:08.472452Z","iopub.status.idle":"2023-04-03T20:41:10.166910Z","shell.execute_reply.started":"2023-04-03T20:41:08.472416Z","shell.execute_reply":"2023-04-03T20:41:10.165773Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The above word cloud shows the most frequent words for each of our target categories.  While some trends start appearing, the data overall seems 'messy' with an opportunity to clean it up for greater insight and accuracy.  Below we will clean the data by applying lower case and removing:\n\n* Punctation\n* Common stopwords  \n* Words less than 4 letters\n* Non-alphabet characters","metadata":{}},{"cell_type":"code","source":"#remove punctuation\ndef remove_punctuation(x):\n    return x.translate(str.maketrans('', '', string.punctuation))\n\n#remove stopwords\ndef remove_stopwords(x):\n    return ' '.join([i for i in x.split() if i not in wordcloud.STOPWORDS])\n\n#remove words less than 4 letters\ndef remove_less_than(x):\n    return ' '.join([i for i in x.split() if len(i) > 3])\n\n#remove words with non-alphabet characters\ndef remove_non_alphabet(x):\n    return ' '.join([i for i in x.split() if i.isalpha()])\n","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:15.895516Z","iopub.execute_input":"2023-04-03T20:41:15.895899Z","iopub.status.idle":"2023-04-03T20:41:15.902855Z","shell.execute_reply.started":"2023-04-03T20:41:15.895857Z","shell.execute_reply":"2023-04-03T20:41:15.901692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['clean_text'] = train_data['text'].apply(lambda x: x.lower())\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_less_than)\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_non_alphabet)\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_stopwords)\ntrain_data['clean_text'] = train_data['clean_text'].apply(remove_punctuation)\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:19.725809Z","iopub.execute_input":"2023-04-03T20:41:19.727015Z","iopub.status.idle":"2023-04-03T20:41:19.838313Z","shell.execute_reply.started":"2023-04-03T20:41:19.726951Z","shell.execute_reply":"2023-04-03T20:41:19.836868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:22.466652Z","iopub.execute_input":"2023-04-03T20:41:22.467046Z","iopub.status.idle":"2023-04-03T20:41:22.480427Z","shell.execute_reply.started":"2023-04-03T20:41:22.467011Z","shell.execute_reply":"2023-04-03T20:41:22.479158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_cases = \"\".join(train_data[train_data['target'] == 0]['clean_text'].values)\nnegative_cases = \"\".join(train_data[train_data['target'] == 1]['clean_text'].values)\n\nfig, axs = plt.subplots(2, 1, figsize=(20, 8))\n\nwc1 = WordCloud(background_color='white').generate(positive_cases)\nwc2 = WordCloud(background_color='white').generate(negative_cases)\n\naxs[0].imshow(wc1, interpolation='bilinear')\naxs[0].set_title('Positive')\naxs[1].imshow(wc2, interpolation='bilinear')\naxs[1].set_title('Negative')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:24.935722Z","iopub.execute_input":"2023-04-03T20:41:24.936151Z","iopub.status.idle":"2023-04-03T20:41:26.391759Z","shell.execute_reply.started":"2023-04-03T20:41:24.936113Z","shell.execute_reply":"2023-04-03T20:41:26.390532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"With the result of the text cleanup, the above word clouds provide greater insight to the words driving the target characterisation.   The clarity of the cleansed text will be helpful in the below model.  While there are additional opportunities to address mis-spellings and abbreviations within the text further, the current cleansed text should suffice within the scope of the assignment.","metadata":{}},{"cell_type":"markdown","source":"# **Model Architecture**","metadata":{}},{"cell_type":"markdown","source":"We will build a long short term memory network (LSTM) model which is a variety of recurrent neural network discussed during the class lecture.  The LSTM architecture provides a short-term memory for RNN that can last thousands of timesteps, thus \"long short-term memory\". The intuition behind the LSTM architecture is to create an additional module in a neural network that effectively learns which information might be needed later on in a sequence and when that information is no longer needed.  This makes it a good architecture for natural language processing where we need the network to learn grammatical dependencies.","metadata":{}},{"cell_type":"markdown","source":"First, we will tokenize each text sentence. Tokenization will break down the sentences into individual words in order to create a matrix of the relationship between those words forming our corpus.","metadata":{}},{"cell_type":"code","source":"max_features=3000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(train_data['clean_text'].values)\nx = tokenizer.texts_to_sequences(train_data['clean_text'].values)\nx = pad_sequences(x)\nx.shape","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:41:30.578715Z","iopub.execute_input":"2023-04-03T20:41:30.579170Z","iopub.status.idle":"2023-04-03T20:41:30.818704Z","shell.execute_reply.started":"2023-04-03T20:41:30.579127Z","shell.execute_reply":"2023-04-03T20:41:30.817564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that can accelerate TensorFlow models with potentially no source code changes.\n# reference: https://www.tensorflow.org/xla\nos.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:42:01.996115Z","iopub.execute_input":"2023-04-03T20:42:01.996579Z","iopub.status.idle":"2023-04-03T20:42:02.002221Z","shell.execute_reply.started":"2023-04-03T20:42:01.996535Z","shell.execute_reply":"2023-04-03T20:42:02.000878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Next we will build the model:\n\n* Embedding layer is the first layer of neural network with 3 parameters:\n    * Number of distinct token vectors\n    * Dimension of embedding vector\n    * Size of the input layer\n    \n\n* A dropout layer is avoid overfitting.\n\n* Activation Function of the Dense layer: Sigmoid function for the binary classification of the target column \n\n* Loss Function : Binary_crossentropy function computes the cross-entropy loss between true labels and predicted labels.\n\n* Optimizer : Adam with the default learning rate of Adam is 0.001","metadata":{}},{"cell_type":"code","source":"embed_dim = 32\nlstm_out = 32\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = x.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(lstm_out, dropout=0.1, recurrent_dropout=0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nadam = optimizers.Adam(learning_rate=0.002)\n#adam= optimizers.Adam()\nmodel.compile(loss = 'binary_crossentropy', optimizer=adam ,metrics = ['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:48:24.898507Z","iopub.execute_input":"2023-04-03T20:48:24.898961Z","iopub.status.idle":"2023-04-03T20:48:25.103509Z","shell.execute_reply.started":"2023-04-03T20:48:24.898920Z","shell.execute_reply":"2023-04-03T20:48:25.100655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Results and Analysis**","metadata":{}},{"cell_type":"markdown","source":"To evaluate the results, we will split the training data 80% for training the model and 20% to validate the results as shown below:","metadata":{}},{"cell_type":"code","source":"y = train_data['target']\nx_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state = 777)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:48:29.799303Z","iopub.execute_input":"2023-04-03T20:48:29.799725Z","iopub.status.idle":"2023-04-03T20:48:29.808340Z","shell.execute_reply.started":"2023-04-03T20:48:29.799686Z","shell.execute_reply":"2023-04-03T20:48:29.807035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x_train, y_train, epochs = 10, batch_size=64, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:49:34.687439Z","iopub.execute_input":"2023-04-03T20:49:34.687839Z","iopub.status.idle":"2023-04-03T20:49:53.423152Z","shell.execute_reply.started":"2023-04-03T20:49:34.687806Z","shell.execute_reply":"2023-04-03T20:49:53.422156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using the fitted model, we will use it to predict the target on our validation data and evaluate the results:","metadata":{}},{"cell_type":"code","source":"y_pred = model.predict(x_test).round()\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:49:56.244280Z","iopub.execute_input":"2023-04-03T20:49:56.245528Z","iopub.status.idle":"2023-04-03T20:49:56.754042Z","shell.execute_reply.started":"2023-04-03T20:49:56.245478Z","shell.execute_reply":"2023-04-03T20:49:56.752708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confuse = confusion_matrix(y_test,y_pred)\nplt.figure(figsize=(4, 4))\nsns.heatmap(confuse, annot=True, fmt='g', cmap='Blues')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:49:59.289397Z","iopub.execute_input":"2023-04-03T20:49:59.289831Z","iopub.status.idle":"2023-04-03T20:49:59.563938Z","shell.execute_reply.started":"2023-04-03T20:49:59.289791Z","shell.execute_reply":"2023-04-03T20:49:59.562552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:50:01.853060Z","iopub.execute_input":"2023-04-03T20:50:01.854060Z","iopub.status.idle":"2023-04-03T20:50:01.870548Z","shell.execute_reply.started":"2023-04-03T20:50:01.854013Z","shell.execute_reply":"2023-04-03T20:50:01.868688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The confusion matrix visualizes and summarizes the performance of the classification:\n\n* Precision, the ratio between the true positives and all the positives, checks the accuracy of the postive class. Out of all the tweets the the model predicted  would be disaster related 69% actual are.\n* Recall, the ratio between the number of true positives and number of false negatives, is a measure of the model correctly identifying true positives. Out of all the tweets that were related to disasters, the model predicted this outcome for 76%.\n* The f1-score of 76% tells us the model did a pretty good job of predicting if the text was related to true disaster.\n* Overall accuracy is 73%","metadata":{}},{"cell_type":"markdown","source":"We can adjust the model parameters to attempt to improve the results.   Let's try to increase the learning rate and reduce the drop rate:","metadata":{}},{"cell_type":"code","source":"embed_dim = 32\nlstm_out = 32\nmodel2 = Sequential()\nmodel2.add(Embedding(max_features, embed_dim,input_length = x.shape[1]))\nmodel2.add(Dropout(0.3))\nmodel2.add(LSTM(lstm_out, dropout=0.3, recurrent_dropout=0.3))\nmodel2.add(Dense(1,activation='sigmoid'))\nadam = optimizers.Adam(learning_rate=0.10)\nmodel2.compile(loss = 'binary_crossentropy', optimizer=adam ,metrics = ['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:51:40.673682Z","iopub.execute_input":"2023-04-03T20:51:40.674527Z","iopub.status.idle":"2023-04-03T20:51:40.878374Z","shell.execute_reply.started":"2023-04-03T20:51:40.674483Z","shell.execute_reply":"2023-04-03T20:51:40.877409Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model2.fit(x_train, y_train, epochs = 10, batch_size=32, validation_data=(x_test, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:51:45.167408Z","iopub.execute_input":"2023-04-03T20:51:45.167826Z","iopub.status.idle":"2023-04-03T20:52:21.947923Z","shell.execute_reply.started":"2023-04-03T20:51:45.167787Z","shell.execute_reply":"2023-04-03T20:52:21.946871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model2.predict(x_test).round()\nprint(y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:52:25.514404Z","iopub.execute_input":"2023-04-03T20:52:25.515730Z","iopub.status.idle":"2023-04-03T20:52:26.038821Z","shell.execute_reply.started":"2023-04-03T20:52:25.515663Z","shell.execute_reply":"2023-04-03T20:52:26.037483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confuse = confusion_matrix(y_test,y_pred)\nplt.figure(figsize=(4, 4))\nsns.heatmap(confuse, annot=True, fmt='g', cmap='Reds')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:52:28.885398Z","iopub.execute_input":"2023-04-03T20:52:28.885808Z","iopub.status.idle":"2023-04-03T20:52:29.151142Z","shell.execute_reply.started":"2023-04-03T20:52:28.885769Z","shell.execute_reply":"2023-04-03T20:52:29.149919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:52:32.786796Z","iopub.execute_input":"2023-04-03T20:52:32.788014Z","iopub.status.idle":"2023-04-03T20:52:32.802373Z","shell.execute_reply.started":"2023-04-03T20:52:32.787951Z","shell.execute_reply":"2023-04-03T20:52:32.801208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The second model did slightly worse than the first model.   While we can continue to adjust hyperparameters, add layers and further cleanse the data or switch to another model architecture completly like BERT, within the scope of the weekly assignment we will proceed with applying the original model against the test dataset for competition submission.","metadata":{}},{"cell_type":"markdown","source":"# **Submission**","metadata":{}},{"cell_type":"markdown","source":"We will clean the test dataset, fit the model and make predictions for submission:","metadata":{}},{"cell_type":"code","source":"test_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:52:59.829044Z","iopub.execute_input":"2023-04-03T20:52:59.829626Z","iopub.status.idle":"2023-04-03T20:52:59.846776Z","shell.execute_reply.started":"2023-04-03T20:52:59.829572Z","shell.execute_reply":"2023-04-03T20:52:59.845457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_data['clean_text'] = test_data['text'].apply(lambda x: x.lower())\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_less_than)\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_non_alphabet)\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_stopwords)\ntest_data['clean_text'] = test_data['clean_text'].apply(remove_punctuation)\n\nl =50\nmax_features=5000\ntokenizer=Tokenizer(num_words=max_features,split=' ')\ntokenizer.fit_on_texts(train_data['clean_text'].values)\nx = tokenizer.texts_to_sequences(train_data['clean_text'].values)\nx = pad_sequences(x, maxlen =l)\n\ntokenizer.fit_on_texts(train_data['clean_text'].values)\ntest_token = tokenizer.texts_to_sequences(test_data['clean_text'].values)\ntest_token = pad_sequences(test_token, maxlen =l)\n\nprint('done')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:53:02.176800Z","iopub.execute_input":"2023-04-03T20:53:02.177197Z","iopub.status.idle":"2023-04-03T20:53:02.630402Z","shell.execute_reply.started":"2023-04-03T20:53:02.177162Z","shell.execute_reply":"2023-04-03T20:53:02.628811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embed_dim = 100\nlstm_out = 100\nmodel = Sequential()\nmodel.add(Embedding(max_features, embed_dim,input_length = x.shape[1]))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(lstm_out, dropout=0.2, return_sequences=True,recurrent_dropout=0.4))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(lstm_out,dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1,activation='sigmoid'))\nadam = optimizers.Adam(learning_rate=2e-3)\nmodel.compile(loss = 'binary_crossentropy', optimizer=adam ,metrics = ['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:53:08.073800Z","iopub.execute_input":"2023-04-03T20:53:08.074264Z","iopub.status.idle":"2023-04-03T20:53:08.461113Z","shell.execute_reply.started":"2023-04-03T20:53:08.074211Z","shell.execute_reply":"2023-04-03T20:53:08.458999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(x,y, epochs = 10,validation_split = 0.2 ,batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:53:11.976502Z","iopub.execute_input":"2023-04-03T20:53:11.976902Z","iopub.status.idle":"2023-04-03T20:58:01.208005Z","shell.execute_reply.started":"2023-04-03T20:53:11.976867Z","shell.execute_reply":"2023-04-03T20:58:01.206482Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat = model.predict(test_token).round()\nsubmission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmission['target'] = np.round(y_hat).astype('int')\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:58:32.098759Z","iopub.execute_input":"2023-04-03T20:58:32.099194Z","iopub.status.idle":"2023-04-03T20:58:35.268987Z","shell.execute_reply.started":"2023-04-03T20:58:32.099154Z","shell.execute_reply":"2023-04-03T20:58:35.267851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.groupby(['target'])['target'].count().plot(kind='bar', color = 'blue', title='Target - Test Predictions Distribution')\nplt.xlabel('Target')\nplt.ylabel('Count')","metadata":{"execution":{"iopub.status.busy":"2023-04-03T20:58:38.188024Z","iopub.execute_input":"2023-04-03T20:58:38.188436Z","iopub.status.idle":"2023-04-03T20:58:38.415784Z","shell.execute_reply.started":"2023-04-03T20:58:38.188402Z","shell.execute_reply":"2023-04-03T20:58:38.414291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Conclusion**","metadata":{}},{"cell_type":"markdown","source":"In this Kaggle competition, an recurrent neural network model using a long short term memory (LSTM) architecture was creasted to solve the binary classification problem in order to classify which Tweets in our dataset are about real disasters and which are not.\n\nThe LSTM model resulted in an accuracy score of 73%. Further adjustment to hyperparameters including the number of layers and parameters used within in them (number of distinct token vectors, dimension of the embedding vector, size of the input layer), adjusting the learning rate or even refining our input text (spell check, modify the clean up parameters) have the potential to further improve accuracy. Early stopping could also address be utilized to minimize the potential for overfitting.\n\nThe results of this competition, evidence both the complexity and effectiveness of recurrent neural network models. Within only a few layers, the models quickly resulted in greater than 1 million trainable parameters while the use of TensorFlow Keras simplified the forward and backward propagation that was manually calculated in class.","metadata":{}}]}